{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb411162",
   "metadata": {},
   "source": [
    "## Notebook for comparing filternet performance against unfiltered residual masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d619ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import subprocess \n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from torchvision import transforms, utils \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from typing import List, Dict, Tuple \n",
    "from tqdm import tqdm \n",
    "from scipy.spatial.transform import Rotation as R \n",
    "from PIL import Image \n",
    "from skimage import io, transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2b680",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb7e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to cuda\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# Create dataset class\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, path_to_data, augmented=False, num_vids=7, seq_length=5, img_size=256):\n",
    "        \"\"\"\n",
    "        Go through each video folder and build a map from\n",
    "        index i in range [0, N], where N is the total number\n",
    "        of frames in all the videos, to a tuple (j, k), where\n",
    "        j is the video number and k the frame index in that\n",
    "        video. For reference, the videos are stored in the\n",
    "        dataset as:\n",
    "\n",
    "        data:\n",
    "        - test1:\n",
    "          - inputs:\n",
    "            - bd_poses.csv\n",
    "            - bd_twists.csv\n",
    "            - frame0000.png\n",
    "            - frame0001.png\n",
    "            - ...\n",
    "          - targets:\n",
    "            - frame0000.png\n",
    "            - frame0001.png\n",
    "            - ...\n",
    "        - test2:\n",
    "            - ...\n",
    "        - ...\n",
    "        \"\"\"\n",
    "        self.img_size = img_size\n",
    "        self.seq_length = seq_length\n",
    "        self.augmented = augmented\n",
    "        self.num_vids = num_vids\n",
    "        self.path_to_data = path_to_data\n",
    "        self.idx_map: List[Tuple[int, int]] = []\n",
    "        self.poses: Dict[int, np.ndarray] = {}\n",
    "        self.twists: Dict[int, np.ndarray] = {}\n",
    "        for i in tqdm(range(1, self.num_vids + 1)):\n",
    "            # Define path to pose\n",
    "            pose_path = os.path.join(path_to_data, f'test{i}/inputs/bd_poses.csv')\n",
    "            twist_path = os.path.join(path_to_data, f'test{i}/inputs/bd_twists.csv')\n",
    "\n",
    "            # Check that files were opened properly\n",
    "            if not os.path.isfile(pose_path):\n",
    "                raise FileNotFoundError(f\"Missing pose file: {pose_path}\")\n",
    "            if not os.path.isfile(twist_path):\n",
    "                raise FileNotFoundError(f\"Missing twist file: {twist_path}\")\n",
    "\n",
    "            # Get poses\n",
    "            self.poses[i] = pd.read_csv(pose_path).to_numpy()\n",
    "            self.twists[i] = pd.read_csv(twist_path).to_numpy()\n",
    "\n",
    "            # Get number of sequences in this video: num_frames - (seq_length - 1)\n",
    "            num_sequences = (self.poses[i].shape[0] - 1) - (self.seq_length - 1)\n",
    "\n",
    "            # Update index map\n",
    "            video_num = [i] * num_sequences\n",
    "            frame_idx = list(range(0, num_sequences))\n",
    "            self.idx_map.extend(list(zip(video_num, frame_idx)))\n",
    "\n",
    "        self.total_num_sequences = len(self.idx_map)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return length of dataset as computed in __init__() function.\n",
    "        \"\"\"\n",
    "        return self.total_num_sequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Use map built in __init__() to retrieve the image,\n",
    "        pose, and twist directly from the dataset.\n",
    "        This avoids loading the entire dataset which\n",
    "        overwhelms RAM.\n",
    "        \"\"\"\n",
    "        assert idx < self.total_num_sequences\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Return indices for saving processed images \n",
    "        indices_arr = np.zeros((self.seq_length, 2)) \n",
    "\n",
    "        seq_start = True\n",
    "        for i in range(self.seq_length):\n",
    "            # Define path to data\n",
    "            test_idx, frame_idx = self.idx_map[idx]\n",
    "            frame_idx += i\n",
    "            padded_frame_idx = self.to_zero_pad_idx(frame_idx)\n",
    "            input_img_path = os.path.join(self.path_to_data,\n",
    "                                    f'test{test_idx}/inputs/frame{padded_frame_idx}.png')\n",
    "            output_img_path = os.path.join(self.path_to_data,\n",
    "                                    f'test{test_idx}/targets/frame{padded_frame_idx}.png')\n",
    "\n",
    "            # store indices in indices array \n",
    "            indices_arr[i] = np.array([test_idx, frame_idx]) \n",
    "\n",
    "            # Load and process data\n",
    "            input_frame = io.imread(input_img_path)\n",
    "            output_frame = io.imread(output_img_path)\n",
    "            input_frame = torch.from_numpy(self.to_grayscale(input_frame)).float()\n",
    "            output_frame = torch.from_numpy(self.to_grayscale(output_frame)).float()\n",
    "            pose = self.poses[test_idx][frame_idx]\n",
    "            pose = torch.from_numpy(self.pose_vector_from_matrix(pose).reshape(-1, 1)).float()\n",
    "            twist = torch.from_numpy(self.twists[test_idx][frame_idx].reshape(-1, 1)).float()\n",
    "            state = torch.cat((pose, twist), dim=0).view(-1)\n",
    "\n",
    "            # Resize frames\n",
    "            h, w = self.img_size, self.img_size\n",
    "            resize_frame = transforms.Resize((h, w))\n",
    "            output_frame = resize_frame(output_frame.unsqueeze(0))\n",
    "            input_frame = resize_frame(input_frame.unsqueeze(0))\n",
    "\n",
    "            if not self.augmented:\n",
    "                # Initialize sequence\n",
    "                if seq_start:\n",
    "                    in_frame_seq = torch.zeros_like(input_frame).view(1, 1, h, w).repeat((self.seq_length, 1, 1, 1))\n",
    "                    out_frame_seq = torch.zeros_like(output_frame).view(1, 1, h, w).repeat((self.seq_length, 1, 1, 1))\n",
    "                    state_seq = torch.zeros_like(state).unsqueeze(0).repeat((self.seq_length, 1))\n",
    "                    seq_start = False\n",
    "\n",
    "                # Update sequence\n",
    "                in_frame_seq[i] = input_frame.view(1, h, w)\n",
    "                out_frame_seq[i] = output_frame.view(1, h, w)\n",
    "                state_seq[i] = state\n",
    "\n",
    "            # If we want a broadcasted frame, pose, twist tensor of shape (B, T, 1, H, W, 14)\n",
    "            else:\n",
    "                expanded_frame = input_frame.view(1, h, w, 1)\n",
    "                expanded_state = state.view(1, 1, 1, -1).repeat(1, h, w, 1)\n",
    "                augmented_frame = torch.cat((expanded_frame, expanded_state), dim=-1)\n",
    "\n",
    "                # Initialize augmented sequence\n",
    "                if seq_start:\n",
    "                    aug_seq_in = torch.zeros_like(augmented_frame).unsqueeze(0).repeat((self.seq_length, 1, 1, 1, 1))\n",
    "                    aug_seq_out = torch.zeros_like(output_frame).unsqueeze(0).repeat((self.seq_length, 1, 1, 1))\n",
    "                    seq_start = False\n",
    "\n",
    "                # Update augmented sequence\n",
    "                aug_seq_in[i] = augmented_frame\n",
    "                aug_seq_out[i] = output_frame\n",
    "\n",
    "        return {'input': (in_frame_seq, state_seq), 'target': out_frame_seq, 'indices': indices_arr} if not self.augmented else {'input': aug_seq_in, 'target': aug_seq_out, 'indices': indices_arr}\n",
    "\n",
    "    def to_zero_pad_idx(self, idx):\n",
    "        \"\"\"\n",
    "        Convert frame index from regular index to zero-padded index.\n",
    "        e.g. 1 -> 00001, 12 -> 00012\n",
    "        \"\"\"\n",
    "        return f'{idx:05d}'\n",
    "\n",
    "    def pose_vector_from_matrix(self, pose):\n",
    "        \"\"\"\n",
    "        Convert 4x4 pose matrix (as a flattenned length 16 vector) into a position and quaternion length 7 vector.\n",
    "        \"\"\"\n",
    "        pose = pose.reshape(4, 4)\n",
    "        position = pose[:3, 3].reshape(3, 1)\n",
    "        orientation = pose[:3, :3]\n",
    "\n",
    "        quat = R.from_matrix(orientation).as_quat().reshape(-1, 1)\n",
    "        norm_quat = quat / np.linalg.norm(quat)\n",
    "\n",
    "        return np.vstack((position, norm_quat)).reshape(-1)\n",
    "\n",
    "    def to_grayscale(self, image):\n",
    "        \"\"\"\n",
    "        Convert PNG image to grayscale mask\n",
    "        \"\"\"\n",
    "        #TODO: Try float16 type\n",
    "        return (image[..., 0] > 127).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4a09d",
   "metadata": {},
   "source": [
    "## Submodules (Encoders, Decoders, Bottlenecks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee770989",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "################ Encoders #################\n",
    "###########################################\n",
    "class ImgEncoder(nn.Module):\n",
    "    def __init__(self, in_channel=1, hidden_channel=16, out_channel=32, h_in=256, out_dim=1024):\n",
    "        \"\"\"\n",
    "        Define convolutional neural network architecture for compressing a 256 x 256 image into a 1024 embedding vector.\n",
    "        Assumes image is square.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute image shape after convolution\n",
    "        stride = 2\n",
    "        padding_one, padding_two = 7, 1\n",
    "        num_ker_one, num_ker_two = 16, 4\n",
    "        h_out = (h_in + 2 * padding_one - num_ker_one) // stride + 1\n",
    "        h_out = (h_out + 2 * padding_two - num_ker_two) // stride + 1\n",
    "\n",
    "        # Define CNN\n",
    "        linear_in_dim = out_channel * h_out ** 2\n",
    "        linear_out_dim = out_dim\n",
    "        self.conv_stack = nn.Sequential(\n",
    "                            nn.Conv2d(in_channel, hidden_channel, num_ker_one, stride=stride, padding=padding_one), # h_in, w_in = (256, 256); h_out, w_out = (128, 128)\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(hidden_channel, out_channel, num_ker_two, stride=stride, padding=padding_two), # h_in, w_in = (128, 128); h_out, w_out = (64, 64)\n",
    "                            nn.ReLU(),\n",
    "                            nn.Flatten(start_dim=1, end_dim=-1), # Flattens (c_out, h_out, w_out) = (32, 64, 64) into 131072\n",
    "                            nn.Linear(linear_in_dim, linear_out_dim) # Encodes the 131072 length flattened convolved image into a 1024 length embedding vector\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define neural network forward pass\n",
    "        Input has shape (T, B, 1, H, W)\n",
    "        \"\"\"\n",
    "        return self.conv_stack(x[-1])\n",
    "\n",
    "class ImgSeqEncoder(nn.Module):\n",
    "    def __init__(self, in_channel=1, hidden_channel=16, out_channel=32, h_in=256, out_dim=1024):\n",
    "        \"\"\"\n",
    "        Define convolutional neural network architecture for compressing a SEQUENCE of 256 x 256 images into a 1024 embedding vector.\n",
    "        Assumes image is square.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute image shape after convolution\n",
    "        stride = 2\n",
    "        padding_one, padding_two = 7, 1\n",
    "        num_ker_one, num_ker_two = 16, 4\n",
    "        h_out = (h_in + 2 * padding_one - num_ker_one) // stride + 1\n",
    "        h_out = (h_out + 2 * padding_two - num_ker_two) // stride + 1\n",
    "\n",
    "        # Define CNN\n",
    "        linear_in_dim = out_channel * h_out ** 2\n",
    "        linear_out_dim = out_dim\n",
    "        self.conv_stack = nn.Sequential(\n",
    "                            nn.Conv2d(in_channel, hidden_channel, num_ker_one, stride=stride, padding=padding_one), # h_in, w_in = (256, 256); h_out, w_out = (128, 128)\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(hidden_channel, out_channel, num_ker_two, stride=stride, padding=padding_two), # h_in, w_in = (128, 128); h_out, w_out = (64, 64)\n",
    "                            nn.ReLU(),\n",
    "                            nn.Flatten(start_dim=1, end_dim=-1), # Flattens (c_out, h_out, w_out) = (32, 64, 64) into 131072\n",
    "                            nn.Linear(linear_in_dim, linear_out_dim) # Encodes the 131072 length flattened convolved image into a 1024 length embedding vector\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define neural network forward pass\n",
    "        Input has shape (T, B, 1, H, W)\n",
    "        \"\"\"\n",
    "        seq_len, batch_size, _, h, w = x.shape\n",
    "        x = x.reshape(seq_len * batch_size, 1, h, w)\n",
    "        return self.conv_stack(x).reshape(seq_len, batch_size, -1)\n",
    "    \n",
    "class Conv3DImgSeqEncoder(nn.Module):\n",
    "    def __init__(self, in_channel=1, hidden_channel=16, out_channel=32, h_in=256, out_dim=4096):\n",
    "        super().__init__()\n",
    "\n",
    "        # Image height after two conv layers with kernel size 3 and stride 2\n",
    "        h_out = h_in // 8  # Assuming 2x2 stride twice\n",
    "        \n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=in_channel, out_channels=hidden_channel,\n",
    "                      kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3)),  # (B, C=1, T, H=256, W=256) --> (B, 16, T, 128, 128) \n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv3d(in_channels=hidden_channel, out_channels=out_channel,\n",
    "                      kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)), # (B, 16, T, 128, 128) --> (B, 32, T, 64, 64)\n",
    "            nn.ReLU(), \n",
    "            \n",
    "            nn.Conv3d(in_channels=out_channel, out_channels=out_channel, \n",
    "                      kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1)), # (B, 32, T, 64, 64) --> (B, 32, T, 32, 32) \n",
    "            nn.ReLU(), \n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(out_channel * h_out * h_out, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, 1, H, W)\n",
    "        return: (B, T, D_emb)\n",
    "        \"\"\"\n",
    "        T, B, C, H, W = x.shape\n",
    "        x = x.permute(1, 2, 0, 3, 4)  # -> (B, C=1, T, H, W)\n",
    "        x = self.conv_stack(x)        # -> (B, C_out, T, H', W')\n",
    "        B, C_out, T, H_out, W_out = x.shape\n",
    "        x = x.permute(2, 0, 1, 3, 4)  # -> (T, B, C_out, H_out, W_out)\n",
    "        x = x.reshape(T, B, -1)       # -> (T, B, C_out * H_out * W_out)\n",
    "        x = self.linear(x)            # -> (T, B, D_emb)\n",
    "        return x\n",
    "\n",
    "class StateEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=13, out_dim=128):\n",
    "        \"\"\"\n",
    "        Define linear layer to generate a length 128 embedding vector from length 13 pose and twist vector.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define nueral network forward pass\n",
    "        Input has shape (T, B, D) need (B, D) \n",
    "        \"\"\"\n",
    "        return self.linear_layer(x[-1])\n",
    "    \n",
    "class StateSeqEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=13, out_dim=128):\n",
    "        \"\"\"\n",
    "        Define linear layer to generate a sequence of length 128 embedding vectors from length 13 pose and twist vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define nueral network forward pass\n",
    "        \"\"\"\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "\n",
    "###########################################\n",
    "################ Decoders #################\n",
    "###########################################\n",
    "class ImgDecoder(nn.Module):\n",
    "    def __init__(self, in_channels=5, hidden_channels=3, out_channels=1, in_dim=512, hidden_dim=4096):\n",
    "        \"\"\"\n",
    "        Reconstructs the image from the embedding vector. (B, T, in_dim): (B, 5, 512)\n",
    "        Output image size is (256, 256).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute output height\n",
    "        h_in = int(np.sqrt(hidden_dim))\n",
    "        stride = 1\n",
    "        pad_one, pad_two = 34, 71\n",
    "        ker_one, ker_two = 5, 15\n",
    "        self.h_out = (h_in + 2 * pad_one - ker_one) // stride + 1\n",
    "        self.h_out = (self.h_out + 2 * pad_two - ker_two) // stride + 1\n",
    "\n",
    "        # Modules\n",
    "        self.decoder = nn.Sequential(\n",
    "                        nn.Linear(in_dim, hidden_dim), # (B, T, hidden_dim)\n",
    "                        nn.ReLU(),\n",
    "                        nn.Unflatten(dim=-1, unflattened_size=(h_in, h_in)),\n",
    "                        nn.Conv2d(in_channels, hidden_channels, ker_one, stride, pad_one),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(hidden_channels, out_channels, ker_two, stride, pad_two),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input has shape (T, B, D_emb) = (5, 16, 512)\n",
    "        Return predicted last image in sequence (B, 1, h_out, h_out)\n",
    "        It treats the input sequence dimension as an images channels dimension and it convolves\n",
    "        the image back to its original dimension.\n",
    "\n",
    "        TODO: Consider instead of passing (T, B, D_emb) as (T * B, D_emb) and reconstructing as\n",
    "        (T, B, 1, h_out, h_out), pass it as (B, T * D_emb) and reconstruct as (B, 1, h_out, h_out).\n",
    "        \"\"\"\n",
    "        return self.decoder(x.permute(1, 0, 2)) # Convnet expects (B, T, H, W)\n",
    "    \n",
    "class ImgDecoder2(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_channels=32, out_channels=1, in_dim=512, hidden_dim=1024, out_size=256):\n",
    "        super().__init__()\n",
    "        self.out_size = out_size\n",
    "        h_in = int(np.sqrt(hidden_dim))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(dim=-1, unflattened_size=(1, h_in, h_in)),  # (B*T, 1, 32, 32)\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # (B*T, 1, 64, 64)\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # (B*T, 32, 128, 128)\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # (B*T, 16, 256, 256)\n",
    "            nn.Conv2d(hidden_channels, out_channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        seq_len, batch_size, D = x.shape\n",
    "        x = x.reshape(seq_len * batch_size, D)\n",
    "        return self.decoder(x).reshape(seq_len, batch_size, 1, self.out_size, self.out_size)\n",
    "\n",
    "class Conv3DImgSeqDecoder(nn.Module):\n",
    "    def __init__(self, in_dim=4096, out_channels=1, h_out=256):\n",
    "        super().__init__()\n",
    "        self.h_mid = h_out // 8  # 32\n",
    "        self.c_mid = 32\n",
    "\n",
    "        self.linear = nn.Linear(in_dim, self.c_mid * self.h_mid * self.h_mid)\n",
    "\n",
    "        self.deconv_stack = nn.Sequential(\n",
    "            nn.ConvTranspose3d(self.c_mid, self.c_mid, kernel_size=(1, 4, 4), stride=(1, 2, 2), padding=(0, 1, 1)),  # 32→64\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(self.c_mid, 16, kernel_size=(1, 4, 4), stride=(1, 2, 2), padding=(0, 1, 1)),         # 64→128\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(16, out_channels, kernel_size=(1, 4, 4), stride=(1, 2, 2), padding=(0, 1, 1)),       # 128→256\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (T, B, D)\n",
    "        T, B, D = x.shape\n",
    "        x = self.linear(x)                       # (T, B, C*H*W)\n",
    "        x = x.view(T, B, self.c_mid, self.h_mid, self.h_mid)\n",
    "        x = x.permute(1, 2, 0, 3, 4)             # (B, C, T, H, W)\n",
    "        x = self.deconv_stack(x)                 # (B, 1, T, 256, 256)\n",
    "        x = x.permute(2, 0, 1, 3, 4)             # (T, B, 1, 256, 256)\n",
    "        return x\n",
    "\n",
    "\n",
    "###########################################\n",
    "################ Bottlenecks ##############\n",
    "###########################################\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.model = nn.LSTM(in_dim, hidden_dim, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)[0]\n",
    "    \n",
    "class MLP(nn.Module): \n",
    "    def __init__(self, in_dim, hidden_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        ### Create model \n",
    "        out_dim = hidden_dim \n",
    "        layers = [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(1, num_layers): \n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU()])\n",
    "        layers.append(nn.Linear(hidden_dim, out_dim))\n",
    "        self.model = nn.Sequential(*layers) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input has shape (B, D) and decoder Expects (1, B, D) \n",
    "        \"\"\"\n",
    "        x = x\n",
    "        return self.model(x).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b3194",
   "metadata": {},
   "source": [
    "## Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e52450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions \n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self, weight_bce=0.7, weight_dice=0.3, smooth=1.0, weight=250.0):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([weight], device=device))\n",
    "        self.weight_bce = weight_bce\n",
    "        self.weight_dice = weight_dice\n",
    "        self.smooth = smooth  # to avoid division by zero\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # BCEWithLogitsLoss expects raw logits\n",
    "        bce_loss = self.bce(logits, targets)\n",
    "\n",
    "        # Apply sigmoid to get probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "        probs = probs.reshape(-1)\n",
    "        targets = targets.reshape(-1)\n",
    "\n",
    "        intersection = (probs * targets).sum()\n",
    "        dice_score = (2. * intersection + self.smooth) / (\n",
    "            probs.sum() + targets.sum() + self.smooth\n",
    "        )\n",
    "        dice_loss = 1 - dice_score\n",
    "\n",
    "        return self.weight_bce * bce_loss + self.weight_dice * dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe9d79d",
   "metadata": {},
   "source": [
    "## Main Module (FilterNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "792d71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterNet(nn.Module):\n",
    "    def __init__(self, in_dim=4224, hidden_dim=4096, seq_length=3, augmented=False):\n",
    "        \"\"\"\n",
    "        Define LSTM architecture with image and state encoders\n",
    "        Must concatenate image and state embeddings to make a 1024 + 128 length embedding vector for lstm\n",
    "        lstm input dimension is then 1024 + 128 = 1152\n",
    "\n",
    "        conv3d vals: in_dim 4224, hidden_dim 4096\n",
    "        conv2d vals: in_dim 1152, hidden_dim 512\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_length\n",
    "        self.augmented = augmented\n",
    "\n",
    "        # Conv3d approach \n",
    "        self.image_encoder = Conv3DImgSeqEncoder()\n",
    "        self.state_encoder = StateSeqEncoder() \n",
    "        self.lstm = LSTM(in_dim, hidden_dim) \n",
    "        self.image_decoder = Conv3DImgSeqDecoder() \n",
    "        self.loss_fun = BCEDiceLoss()\n",
    "\n",
    "\n",
    "    def loss(self, sequence):\n",
    "        \"\"\"\n",
    "        Unless using the augmented (B, T, 1, H, W, 14) tensor, data will come in as a Tuple storing a sequence of {'input', 'target'}\n",
    "        dictionaries. Each 'input' field contains a frame, a pose, and a twist, each as a tensor.\n",
    "\n",
    "        Must loop through the sequence to generate embedding, but for small sequence lengths, the overhead is negligible, and actually\n",
    "        preferable, than the memory overhead of the augmented tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.augmented:\n",
    "            # Retrieve data\n",
    "            input, output = sequence['input'].permute(1, 0, 2, 3, 4, 5), sequence['target'].permute(1, 0, 2, 3, 4) # switch to sequence first\n",
    "            frames = input[..., 0]\n",
    "            state = input[:, :, :, 0, 0, 1:]\n",
    "            out_frame = output[-1, ...]\n",
    "        else:\n",
    "            # Pass each element of the sequence through the model\n",
    "            frames = sequence['input'][0].permute(1, 0, 2, 3, 4) # (B, T, 1, H, W) --> (T, B, 1, H, W)\n",
    "            state = sequence['input'][1].permute(1, 0, 2) # (B, T, 13) --> (T, B, 13)\n",
    "            out_frame = sequence['target'].permute(1, 0, 2, 3, 4) #(B, T, 1, H, W) --> (T, B, 1, H, W)\n",
    "\n",
    "        # Pass inputs through encoders\n",
    "        img_embedding = self.image_encoder(frames) # out dim should be (T, B, d_img_emb)\n",
    "        state_embedding = self.state_encoder(state) # out dim should be (T, B, d_state_emb)\n",
    "\n",
    "        # Concatenate embedding vector and reconstruct sequence as a tensor\n",
    "        compressed_input = torch.cat((img_embedding, state_embedding), dim=-1) # (T, B, in_dim)\n",
    "\n",
    "        # Pass compressed sequence through LSTM\n",
    "        lstm_out = self.lstm(compressed_input) # should have shape (T, B, hidden_dim) (5, 16, 512)\n",
    "\n",
    "        # Pass through decoder to reconstruct predicted last frame in sequence\n",
    "        pred_frames = self.image_decoder(lstm_out)\n",
    "\n",
    "        # Get loss between predicted frame and last frame in target sequence\n",
    "        return self.loss_fun(pred_frames, out_frame), pred_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c119a6",
   "metadata": {},
   "source": [
    "## Prepare data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e122035f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 125.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FilterNet(\n",
       "  (image_encoder): Conv3DImgSeqEncoder(\n",
       "    (conv_stack): Sequential(\n",
       "      (0): Conv3d(1, 16, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3))\n",
       "      (1): ReLU()\n",
       "      (2): Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (linear): Linear(in_features=32768, out_features=4096, bias=True)\n",
       "  )\n",
       "  (state_encoder): StateSeqEncoder(\n",
       "    (linear_layer): Linear(in_features=13, out_features=128, bias=True)\n",
       "  )\n",
       "  (lstm): LSTM(\n",
       "    (model): LSTM(4224, 4096)\n",
       "  )\n",
       "  (image_decoder): Conv3DImgSeqDecoder(\n",
       "    (linear): Linear(in_features=4096, out_features=32768, bias=True)\n",
       "    (deconv_stack): Sequential(\n",
       "      (0): ConvTranspose3d(32, 32, kernel_size=(1, 4, 4), stride=(1, 2, 2), padding=(0, 1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose3d(32, 16, kernel_size=(1, 4, 4), stride=(1, 2, 2), padding=(0, 1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): ConvTranspose3d(16, 1, kernel_size=(1, 4, 4), stride=(1, 2, 2), padding=(0, 1, 1))\n",
       "    )\n",
       "  )\n",
       "  (loss_fun): BCEDiceLoss(\n",
       "    (bce): BCEWithLogitsLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training parameters\n",
    "num_workers = 0\n",
    "num_vids = 7 \n",
    "path_to_data = '/home/jrached/cv_project_code/project/data/filter_net/processed_flow'\n",
    "augmented = False\n",
    "batch_size = 1\n",
    "seq_length = 3\n",
    "img_size = 256\n",
    "\n",
    "dataset = VideoDataset(path_to_data, augmented=augmented, num_vids=num_vids, seq_length=seq_length, img_size=img_size)\n",
    "\n",
    "val_ratio = 0.2\n",
    "val_size = int(len(dataset) * val_ratio)\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, list(range(train_size)))\n",
    "val_dataset = torch.utils.data.Subset(dataset, list(range(train_size, len(dataset))))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Initialize model and load weights \n",
    "path_to_save = \"/home/jrached/cv_project_code/cv_project/models/filternet1.pt\"\n",
    "model = FilterNet(seq_length=seq_length)\n",
    "model = model.to(device) \n",
    "model.load_state_dict(torch.load(path_to_save, weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde7b9c",
   "metadata": {},
   "source": [
    "## Filter data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02e15e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_path = '/home/jrached/cv_project_code/project/data/filter_net/filtered_flow'\n",
    "data_path = '/home/jrached/cv_project_code/project/data/filter_net/dataset'\n",
    "\n",
    "\n",
    "# Make directories\n",
    "for vid_idx in range(1, num_vids+1): \n",
    "    os.makedirs(os.path.join(dest_path, f'test{vid_idx}', 'mask'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dest_path, f'test{vid_idx}', 'depth'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dest_path, f'test{vid_idx}', 'intrinsics'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dest_path, f'test{vid_idx}', 'poses'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1317a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 7630/17095 [04:14<05:04, 31.05it/s]"
     ]
    }
   ],
   "source": [
    "h, w = 480, 848\n",
    "resize_frame = transforms.Resize((h, w))\n",
    "\n",
    "is_first_seq = True \n",
    "prev_test_idx = 1 \n",
    "for sequence in tqdm(train_loader): \n",
    "    # Move data to device\n",
    "    frames, state = sequence['input']\n",
    "    out_frame = sequence['target']\n",
    "    indices = sequence['indices'].squeeze(0) # Squeeze batch dimension \n",
    "\n",
    "    sequence['input'] = (frames.to(device), state.to(device))\n",
    "    sequence['target'] = out_frame.to(device)\n",
    "\n",
    "    # Compute the loss and its gradients\n",
    "    loss, pred  = model.loss(sequence)\n",
    "\n",
    "    test_idx = int(indices[0][0].item())\n",
    "    if prev_test_idx != test_idx: \n",
    "        is_first_seq = True \n",
    "        prev_test_idx = test_idx \n",
    "\n",
    "    if is_first_seq: \n",
    "        # If first sequence, save full sequence\n",
    "        for i, index in enumerate(indices): \n",
    "            test_idx, frame_idx = int(index[0].item()), int(index[1].item()) \n",
    "\n",
    "            prob_pred_frame = torch.sigmoid(pred[i])\n",
    "            out = (prob_pred_frame > 0.5).to(torch.float32) \n",
    "            output_frame = resize_frame(out[0, :, :, :])\n",
    "            image = Image.fromarray((output_frame[0, :, :] * 255).cpu().detach().numpy().astype(np.uint8))\n",
    "            image.save(os.path.join(dest_path, f'test{test_idx}/mask/frame{frame_idx}.png'))\n",
    "            subprocess.run(['cp', os.path.join(data_path, f'test{test_idx}/depth/depth_img{frame_idx}.png'), os.path.join(dest_path, f'test{test_idx}/depth/depth_img{frame_idx}.png')])\n",
    "\n",
    "        subprocess.run(f'cp {data_path}/test{test_idx}/poses/* {dest_path}/test{test_idx}/poses/.', shell=True)\n",
    "        subprocess.run(f'cp {data_path}/test{test_idx}/intrinsics/* {dest_path}/test{test_idx}/intrinsics/.', shell=True)\n",
    "        is_first_seq = False \n",
    "    else: \n",
    "        # If not first sequence, only save last element \n",
    "        pred = pred[-1] \n",
    "        test_idx, frame_idx = int(indices[-1][0].item()), int(indices[-1][1].item()) \n",
    "\n",
    "        prob_pred_frame = torch.sigmoid(pred)\n",
    "        out = (prob_pred_frame > 0.5).to(torch.float32) \n",
    "        output_frame = resize_frame(out[0, :, :, :])\n",
    "        image = Image.fromarray((output_frame[0, :, :] * 255).cpu().detach().numpy().astype(np.uint8))\n",
    "        image.save(os.path.join(dest_path, f'test{test_idx}/mask/frame{frame_idx}.png'))\n",
    "        subprocess.run(['cp', os.path.join(data_path, f'test{test_idx}/depth/depth_img{frame_idx}.png'), os.path.join(dest_path, f'test{test_idx}/depth/depth_img{frame_idx}.png')])\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f26b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4273/4273 [02:48<00:00, 25.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# h, w = 480, 848\n",
    "# resize_frame = transforms.Resize((h, w))\n",
    "\n",
    "# is_first_seq = True \n",
    "# prev_test_idx = 1 \n",
    "# for sequence in tqdm(val_loader): \n",
    "#     # Move data to device\n",
    "#     frames, state = sequence['input']\n",
    "#     out_frame = sequence['target']\n",
    "#     indices = sequence['indices'].squeeze(0) # Squeeze batch dimension \n",
    "\n",
    "#     sequence['input'] = (frames.to(device), state.to(device))\n",
    "#     sequence['target'] = out_frame.to(device)\n",
    "\n",
    "#     # Compute the loss and its gradients\n",
    "#     loss, pred  = model.loss(sequence)\n",
    "\n",
    "#     test_idx = int(indices[0][0].item())\n",
    "#     if prev_test_idx != test_idx: \n",
    "#         is_first_seq = True \n",
    "#         prev_test_idx = test_idx \n",
    "\n",
    "#     if is_first_seq: \n",
    "#         # If first sequence, save full sequence\n",
    "#         for i, index in enumerate(indices): \n",
    "#             test_idx, frame_idx = int(index[0].item()), int(index[1].item()) \n",
    "\n",
    "#             prob_pred_frame = torch.sigmoid(pred[i])\n",
    "#             out = (prob_pred_frame > 0.5).to(torch.float32) \n",
    "#             output_frame = resize_frame(out[0, :, :, :])\n",
    "#             image = Image.fromarray((output_frame[0, :, :] * 255).cpu().detach().numpy().astype(np.uint8))\n",
    "#             image.save(os.path.join(dest_path, f'test{test_idx}/mask/frame{frame_idx}.png'))\n",
    "#             subprocess.run(['cp', os.path.join(data_path, f'test{test_idx}/depth/depth_img{frame_idx}.png'), os.path.join(dest_path, f'test{test_idx}/depth/depth_img{frame_idx}.png')])\n",
    "\n",
    "#         subprocess.run(f'cp {data_path}/test{test_idx}/poses/* {dest_path}/test{test_idx}/poses/.', shell=True)\n",
    "#         subprocess.run(f'cp {data_path}/test{test_idx}/intrinsics/* {dest_path}/test{test_idx}/intrinsics/.', shell=True)\n",
    "#         is_first_seq = False \n",
    "#     else: \n",
    "#         # If not first sequence, only save last element \n",
    "#         pred = pred[-1] \n",
    "#         test_idx, frame_idx = int(indices[-1][0].item()), int(indices[-1][1].item()) \n",
    "\n",
    "#         prob_pred_frame = torch.sigmoid(pred)\n",
    "#         out = (prob_pred_frame > 0.5).to(torch.float32) \n",
    "#         output_frame = resize_frame(out[0, :, :, :])\n",
    "#         image = Image.fromarray((output_frame[0, :, :] * 255).cpu().detach().numpy().astype(np.uint8))\n",
    "#         image.save(os.path.join(dest_path, f'test{test_idx}/mask/frame{frame_idx}.png'))\n",
    "#         subprocess.run(['cp', os.path.join(data_path, f'test{test_idx}/depth/depth_img{frame_idx}.png'), os.path.join(dest_path, f'test{test_idx}/depth/depth_img{frame_idx}.png')])\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a3427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6fbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
